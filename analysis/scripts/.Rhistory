s<-NGramTokenizer(a,Weka_control(min=2 , max=2))
s<-NGramTokenizer(a,Weka_control(min=2 , max=2))
NGramTokenizer = function(x) NGramTokenizer(x,Weka_control(min=2 , max=2))
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed "))
test <- DocumentTermMatrix(a, control = list(tokenize = NGramTokenizer))
test <- as.matrix(test)
View(test)
test <- as.matrix(table(test))
test <- DocumentTermMatrix(a, control = list(tokenize = NGramTokenizer))
----------------------------------------------------------
NGramTokenizer = function(x) NGramTokenizer(x,Weka_control(min=2 , max=2))
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed "))
test <- DocumentTermMatrix(a, control = list(tokenize = NGramTokenizer))
test <- as.matrix(table(test))
----------------------------------------------------------
NGramTokenizer = function(x) NGramTokenizer(x,Weka_control(min=2 , max=2))
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed "))
test <- DocumentTermMatrix(a, control = list(tokenize = NGramTokenizer))
test <- as.matrix(table(test))
NGramTokenizer = function(x) NGramTokenizer(x,Weka_control(min=2 , max=2))
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed "))
test <- DocumentTermMatrix(a, control = list(tokenize = NGramTokenizer))
test <- as.matrix(table(test))
test <- as.data.frame(table(test))
test1 <- data.frame(table(test))
test <- DocumentTermMatrix(a, control = list(tokenize = NGramTokenizer))
test1 <- data.frame(table(test))
gc()
gc()
gc()
gc()
install.packages("qunteda")
NGramTokenizer = function(x) NGramTokenizer(x,Weka_control(n=2))
test <- DocumentTermMatrix(a, control = list(tokenize = NGramTokenizer))
load(file = "/home/max/Desktop/test/bbc_DataMatrix.RData")
bbc.data.matrix$content <-iconv(bbc.data.matrix$content,"WINDOWS-1252","UTF-8")
#--------------------------- prepare training_dataset and test1_dataset , test2_dataset -----------------
set.seed(100) # for randmnes
trian <- createDataPartition(y=bbc.data.matrix$class,p=0.70 , list = FALSE)
train_dataset <- bbc.data.matrix[trian,]
testdataset <- bbc.data.matrix[-trian,]
#---------------------------------------------------------------
NGramTokenizer = function(x) NGramTokenizer(x,Weka_control(n=2))
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed  no one will know"))
test <- DocumentTermMatrix(a, control = list(tokenize = NGramTokenizer))
#-------------------------------------------------------------------------------
test
train.tokens <- as.matrix(test)
test <- DocumentTermMatrix(a, control = list(tokenize = NGramTokenizer))
NGramTokenizer = function(x) NGramTokenizer(x,Weka_control(min=3 , max=3))
NGramTokenizer = function(x) NGramTokenizer(x,Weka_control(min=3 , max=3))
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed no one will know sasda wwewe cfdfsdfs"))
library(SnowballC)
library(textreadr)
library(wordcloud)
library(rpart)
library(caret)
library(tm)
library(MASS)
library(RWeka)
library(rminer)
library(kernlab)
load(file = "/home/max/Desktop/test/bbc_DataMatrix.RData")
bbc.data.matrix$content <-iconv(bbc.data.matrix$content,"WINDOWS-1252","UTF-8")
#--------------------------- prepare training_dataset and test1_dataset , test2_dataset -----------------
set.seed(100) # for randmnes
trian <- createDataPartition(y=bbc.data.matrix$class,p=0.70 , list = FALSE)
train_dataset <- bbc.data.matrix[trian,]
testdataset <- bbc.data.matrix[-trian,]
#---------------------------------------------------------------
NGramTokenizer = function(x) NGramTokenizer(x,Weka_control(min=3 , max=3))
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed no one will know sasda wwewe cfdfsdfs"))
test <- DocumentTermMatrix(a, control = list(tokenize = NGramTokenizer))
#-------------------------------------------------------------------------------
test
train.tokens <- as.matrix(test)
View(train.tokens)
NGramTokenizer = function(x) NGramTokenizer(x,Weka_control(n=2))
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed no one will know sasda wwewe cfdfsdfs"))
test <- DocumentTermMatrix(a, control = list(tokenize = NGramTokenizer))
train.tokens <- as.matrix(test)
test
View(train.tokens)
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed no one will know sasda wwewe cfdfsdfs"))
#--------------------------- prepare training_dataset and test1_dataset , test2_dataset -----------------
set.seed(100) # for randmnes
trian <- createDataPartition(y=bbc.data.matrix$class,p=0.70 , list = FALSE)
train_dataset <- bbc.data.matrix[trian,]
testdataset <- bbc.data.matrix[-trian,]
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed no one will know sasda wwewe cfdfsdfs"))
test <- DocumentTermMatrix(a, control = list(tokenize = TrigramTokenizer))
x <-tokenizers::tokenize_ngrams(a,2)
x
x <-tokenizers::tokenize_ngrams(x = a , lowercase = TRUE, n = 2)
test <- DocumentTermMatrix(a, control = list(tokenize = x))
train.tokens <- as.matrix(test)
View(train.tokens)
train.tokens
x
test <- TermDocumentMatrix(a, control = list(tokenize = x))
train.tokens <- as.matrix(test)
View(train.tokens)
x <-tokenizers::tokenize_ngrams(x = a , lowercase = TRUE, n = 2 , ngram_delim = "-")
x
test <- DocumentTermMatrix(a, control = list(tokenize = x))
train.tokens <- as.matrix(test)
test
View(train.tokens)
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed no one will know sasda wwewe cfdfsdfs"))
x <-tokenizers::tokenize_ngrams(x = a , lowercase = TRUE, n = 2 , ngram_delim = "_")
test <- DocumentTermMatrix(a, control = list(tokenize = x))
#-------------------------------------------------------------------------------
test
train.tokens <- as.matrix(test)
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed no one will know sasda wwewe cfdfsdfs"))
x <-tokenizers::tokenize_ngrams(x = a , lowercase = TRUE, n = 2 , ngram_delim = "_")
test <- DocumentTermMatrix(a, control = list(tokenize = x))
#-------------------------------------------------------------------------------
test
train.tokens <- as.matrix(test)
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed no one will know sasda wwewe cfdfsdfs"))
x <-tokenizers::tokenize_ngrams(x = a , lowercase = TRUE, n = 2 , ngram_delim = "_")
test <- DocumentTermMatrix(a, control = list(tokenize = x))
#-------------------------------------------------------------------------------
test
train.tokens <- as.matrix(test)
View(train.tokens)
test$dimnames
test$i
test$
train.tokens <- as.matrix(test)
x <-tokenizers::tokenize_ngrams(x = train_dataset, n = 2 , ngram_delim = "_")
train_corpus <- (VectorSource(train_dataset))
x <-tokenizers::tokenize_ngrams(x = train_corpus, n = 2 , ngram_delim = "_")
train_corpus
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
FourgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
test <- DocumentTermMatrix(a, control = list(tokenize = BigramTokenizer()))
test <- DocumentTermMatrix(a, control = list(tokenize = BigramTokenizer)
#-------------------------------------------------------------------------------
train.tokens <- as.matrix(test)
test <- DocumentTermMatrix(a, control = list(tokenize = BigramTokenizer))
a <- Corpus(VectorSource("we are not alone ahmed  fuck you ahmed no one will know sasda wwewe cfdfsdfs"))
train_corpus <- (VectorSource(train_dataset))
test <- DocumentTermMatrix(a, control = list(tokenize = BigramTokenizer))
#-------------------------------------------------------------------------------
train.tokens <- as.matrix(test)
View(train.tokens)
x <- ngram_tokenizer(2)(a)
ngram_tokenizer <- function(n = 1L, skip_word_none = TRUE) {
stopifnot(is.numeric(n), is.finite(n), n > 0)
options <- stringi::stri_opts_brkiter(type="word", skip_word_none = skip_word_none)
function(x) {
stopifnot(is.character(x))
# Split into word tokens
tokens <- unlist(stringi::stri_split_boundaries(x, opts_brkiter=options))
len <- length(tokens)
if(all(is.na(tokens)) || len < n) {
# If we didn't detect any words or number of tokens is less than n return empty vector
character(0)
} else {
sapply(
1:max(1, len - n + 1),
function(i) stringi::stri_join(tokens[i:min(len, i + n - 1)], collapse = " ")
)
}
}
}
x <- ngram_tokenizer(2)(a)
x <- ngram_tokenizer(2)(train_dataset$content)
x
test <- DocumentTermMatrix(train_corpus, control = list(tokenize =x))
test <- DocumentTermMatrix(train_corpus, control = list(tokenize =x))
test <- DocumentTermMatrix(train_corpus, control = list(tokenize =x))
test <- DocumentTermMatrix(train_corpus, control = list(tokenize =x))
test <- DocumentTermMatrix(train_corpus, control = list(tokenize =x))
test <- DocumentTermMatrix(train_corpus, control = list(tokenize =x))
test <- DocumentTermMatrix(train_corpus, control = list(tokenize =x))
install.packages("NLP")
install.packages("NLP")
library(NLP)
library(NLP)
library(NLP)
install.packages("NLP")
install.packages("NLP")
install.packages(c("biclust", "cairoDevice", "caret", "Cubist", "CVST", "data.table", "etm", "highr", "htmlTable", "labelled", "later", "mice", "munsell", "mvtnorm", "partykit", "pdftools", "pillar", "plotmo", "plotrix", "progress", "purrr", "RcppRoll", "RGtk2", "rJava", "rlang", "rmarkdown", "RWeka", "stringi", "textshape", "tidytext", "utf8", "visNetwork", "xgboost", "zoo"))
install.packages(c("bindr", "bindrcpp", "caret", "CVST", "ddalpha", "dplyr", "DRR", "kernlab", "lava", "lubridate", "munsell", "pillar", "prodlim", "purrr", "Rcpp", "RcppRoll", "recipes", "reshape2", "rJava", "rlang", "robustbase", "sfsmisc", "stringi", "stringr", "tibble", "utf8", "withr"), lib="/usr/lib/R/site-library")
install.packages(c("cluster", "foreign", "MASS", "Matrix", "nlme", "survival"), lib="/usr/lib/R/library")
install.packages(c("cluster", "foreign", "MASS", "Matrix", "nlme", "survival"), lib = "/usr/lib/R/library")
install.packages(c("cluster", "foreign", "MASS", "Matrix", "nlme", "survival"), lib = "/usr/lib/R/library")
install.packages(c("cluster", "foreign", "MASS", "Matrix", "nlme", "survival"), lib = "/usr/lib/R/library")
install.packages(c("cluster", "foreign", "MASS", "Matrix", "nlme", "survival"), lib = "/usr/lib/R/library")
install.packages(c("cluster", "foreign", "MASS", "Matrix", "nlme", "survival"), lib = "/usr/lib/R/library")
install.packages(c("cluster", "foreign", "MASS", "Matrix", "nlme", "survival"), lib = "/usr/lib/R/library")
install.packages(c("cluster", "foreign", "MASS", "Matrix", "nlme", "survival"), lib = "/usr/lib/R/library")
install.packages(c("cluster", "foreign", "MASS", "Matrix", "nlme", "survival"), lib = "/usr/lib/R/library")
install.packages(c("cluster", "foreign", "MASS", "Matrix", "nlme", "survival"), lib = "/usr/lib/R/library")
load(file = "/home/max/Desktop/test/bbc_DataMatrix.RData")
View(bbc.data.matrix)
load("/home/max/Desktop/best_sample.RData")
prop.table(table(trainData))
library(caret)
library(doParallel)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
timr <- Sys.time()
svm <- train(y~.,data = trainData,method = 'svmLinear3')
## When you are done:
stopCluster(cl)
total_runtime <- difftime(Sys.time(), timr)
print(total_runtime)
pre <- predict(svm,testData[,-1])
t <- table(pre,testData$y)
acc = sum(diag(t))/sum(t)
BagOW <- findFreqTerms(trainData)
preProcess_TFIDF <- function(row.data, stopword.dir, BagOfWord , boolstemm ){
packages <- c('tm')
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(packages, rownames(installed.packages())))
}
library(tm)
row.data<-iconv(row.data,"WINDOWS-1252","UTF-8")
stopwordlist<- readLines(stopword.dir)
train_corpus<- Corpus(VectorSource(row.data))
train_corpus <-tm_map(train_corpus,content_transformer(tolower))
train_corpus<-tm_map(train_corpus,removeNumbers)
train_corpus<-tm_map(train_corpus,removeWords,stopwords("english"))
train_corpus<-tm_map(train_corpus,removeWords,stopwordlist)
train_corpus<- tm_map(train_corpus,removePunctuation)
train_corpus<-tm_map(train_corpus,stripWhitespace)
if(boolstemm){
train_corpus<-tm_map(train_corpus,stemDocument,language = "english")
}
DTM <-DocumentTermMatrix(train_corpus,
control = list(tolower = T ,
removeNumbers =T ,
removePunctuation = T ,
stopwords = T
, stripWhitespace = T ,
dictionary = BagOfWord,
weighting = weightTfIdf))
print("DTM DONE !!")
print(DTM)
return(DTM)
}
binary.weight <-function(x)
{
x <- (x > 0)+0
}
BagOW <- findFreqTerms(trainData)
library(SnowballC)
library(textreadr)
library(wordcloud)
library(rpart)
library(caret)
library(tm)
library(MASS)
#library(RWeka)
library(rminer)
library(kernlab)
library(SnowballC)
library(textreadr)
library(wordcloud)
library(rpart)
library(caret)
library(tm)
library(MASS)
#library(RWeka)
library(rminer)
library(kernlab)
BagOW <- findFreqTerms(trainData)
BagOW <- findFreqTerms(as.DocumentTermMatrix(trainData))
BagOW <- findFreqTerms(as.DocumentTermMatrix(as.matrix(trainData)))
BagOW <- findFreqTerms(as.DocumentTermMatrix(trainData ,weighting = 1))
BagOW <- findFreqTerms(as.DocumentTermMatrix(trainData ,weighting = "TFIDF"))
BagOW <- findFreqTerms(as.DocumentTermMatrix(trainData ,weighting = weightTfIdf))
BagOW <- findFreqTerms(as.DocumentTermMatrix(x = trainData ,weighting = weightTfIdf))
BagOW <- findFreqTerms(as.DocumentTermMatrix(x = as.matrix(trainData) ,weighting = weightTfIdf))
dim(trainData)
bag
bagm<-colnames(trainData)
bagm<-colnames(trainData[-1])
exdata <- read.dir("/home/max/Desktop/ex/",".TXT")
setwd("/home/max/Desktop/Identification of the class of English text among several classes/analysis/scripts/")
exdata <- read.dir("/home/max/Desktop/ex/",".TXT")
exdata <- read.dir("/home/max/Desktop/ex/",".TXT")
read.dir <-function(dir , pattern){
file.names <- dir(dir, pattern = pattern)
file.names = as.data.frame(file.names)
file.names$content = NA
colnames(file.names) = c("filename", "content" )
for(i in 1:length(file.names[,1])){
path <- paste0(dir,'/',file.names[i,1])
line <-  readLines(path)
# print(file.names[i,1])
file.names[i,2] <-paste(line, sep = "", collapse = "")
}
return(file.names)
}
exdata <- read.dir("/home/max/Desktop/ex/",".TXT")
exdata$content <- iconv(exdata$content,"WINDOWS-1252","UTF-8")
ex_dtm <-preProcess_TFIDF(row.data = exdata$content ,stopword.dir = "test.txt",BagOfWord = BagOW , TRUE )
ex_dtm <-preProcess_TFIDF(row.data = exdata$content ,stopword.dir = "test.txt",BagOfWord = bagm , TRUE )
ex_dtm <-preProcess_TFIDF(row.data = exdata$content ,stopword.dir = "test.txt",BagOfWord = bagm , TRUE )
str(ex_dtm)
dim(ex_dtm)
ex_matrix <- as.matrix(ex_dtm)
ex_data_model <- as.data.frame(ex_matrix)
dim(ex_data_model)
ex_pred = predict(svm,newdata = ex_data_model )
for(i in 1:length(ex_corpus) ){
print( ex_corpus[[i]][["content"]] )
print(": ")
print( ex_pred[i])
}
ex_corpus <- Corpus(VectorSource(as.matrix(exdata$content)))
for(i in 1:length(ex_corpus) ){
print( ex_corpus[[i]][["content"]] )
print(": ")
print( ex_pred[i])
}
ex
ex_pred
ex_pred
ex_dtm <-preProcess_TFIDF(row.data = exdata$content ,stopword.dir = "test.txt",BagOfWord = bagm , TRUE )
read.dir <-function(dir , pattern){
file.names <- dir(dir, pattern = pattern)
file.names = as.data.frame(file.names)
file.names$content = NA
colnames(file.names) = c("filename", "content" )
for(i in 1:length(file.names[,1])){
path <- paste0(dir,'/',file.names[i,1])
line <-  readLines(path)
# print(file.names[i,1])
file.names[i,2] <-paste(line, sep = "", collapse = "")
}
return(file.names)
}
library(SnowballC)
library(textreadr)
library(wordcloud)
library(rpart)
library(caret)
library(tm)
library(MASS)
#library(RWeka)
library(rminer)
library(kernlab)
load(file = "/home/max/Desktop/test/bbc_DataMatrix.RData")
preProcess_TFIDF <- function(row.data, stopword.dir, BagOfWord , boolstemm ){
packages <- c('tm')
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(packages, rownames(installed.packages())))
}
library(tm)
row.data<-iconv(row.data,"WINDOWS-1252","UTF-8")
stopwordlist<- readLines(stopword.dir)
train_corpus<- Corpus(VectorSource(row.data))
train_corpus <-tm_map(train_corpus,content_transformer(tolower))
train_corpus<-tm_map(train_corpus,removeNumbers)
train_corpus<-tm_map(train_corpus,removeWords,stopwords("english"))
train_corpus<-tm_map(train_corpus,removeWords,stopwordlist)
train_corpus<- tm_map(train_corpus,removePunctuation)
train_corpus<-tm_map(train_corpus,stripWhitespace)
if(boolstemm){
train_corpus<-tm_map(train_corpus,stemDocument,language = "english")
}
DTM <-DocumentTermMatrix(train_corpus,
control = list(tolower = T ,
removeNumbers =T ,
removePunctuation = T ,
stopwords = T
, stripWhitespace = T ,
dictionary = BagOfWord,
weighting = weightTfIdf))
print("DTM DONE !!")
print(DTM)
return(DTM)
}
binary.weight <-function(x)
{
x <- (x > 0)+0
}
train_dtm <-preProcess_TFIDF(row.data = bbc.data.matrix$content ,stopword.dir = "test.txt",BagOfWord = NULL , TRUE)
train_dtm <- removeSparseTerms(train_dtm,0.993)
BagOW <- findFreqTerms(train_dtm)
dim(train_dtm)
train_matrix <- as.matrix(train_dtm)
#train_matrix <- binary.weight(train_matrix)
train_data_model <- as.data.frame(train_matrix)
train_data_model <- data.frame(y=bbc.data.matrix$class , x = train_data_model)
library(caret)
library(doParallel)
acc_matrix <- matrix(nrow = 10 , ncol = 2 )
colnames(acc_matrix) <- c("accuracy" , "Time")
#Randomly shuffle the data
train_data_model<-train_data_model[sample(nrow(train_data_model)),]
#Create 10 equally size folds
folds <- cut(seq(1,nrow(train_data_model)),breaks=10,labels=FALSE)
#Perform 10 fold cross validation
for(i in 1:10){
#Segement your data by fold using the which() function
testIndexes <- which(folds==i,arr.ind=TRUE)
testData <- train_data_model[testIndexes, ]
trainData <- train_data_model[-testIndexes, ]
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
timr <- Sys.time()
svm <- train(y~.,data = trainData,method = 'svmLinear3')
## When you are done:
stopCluster(cl)
total_runtime <- difftime(Sys.time(), timr)
print(total_runtime)
pre <- predict(svm,testData[,-1])
t <- table(pre,testData$y)
acc = sum(diag(t))/sum(t)
print(acc)
acc_matrix[i,1] = acc
acc_matrix[i,2] = total_runtime
}
View(acc_matrix)
save(trainData,testData,BagOW,file = "/home/max/Desktop/best_sample.RData")
load("/home/max/Desktop/best_sample.RData")
:  data frame
# ====================================================================================================
read.dir <-function(dir , pattern){
file.names <- dir(dir, pattern = pattern)
file.names = as.data.frame(file.names)
file.names$content = NA
colnames(file.names) = c("filename", "content" )
for(i in 1:length(file.names[,1])){
path <- paste0(dir,'/',file.names[i,1])
line <-  readLines(path)
# print(file.names[i,1])
file.names[i,2] <-paste(line, sep = "", collapse = "")
}
return(file.names)
}
preProcess_TFIDF <- function(row.data, stopword.dir, BagOfWord , boolstemm ){
packages <- c('tm')
if (length(setdiff(packages, rownames(installed.packages()))) > 0) {
install.packages(setdiff(packages, rownames(installed.packages())))
}
library(tm)
row.data<-iconv(row.data,"WINDOWS-1252","UTF-8")
stopwordlist<- readLines(stopword.dir)
train_corpus<- Corpus(VectorSource(row.data))
train_corpus <-tm_map(train_corpus,content_transformer(tolower))
train_corpus<-tm_map(train_corpus,removeNumbers)
train_corpus<-tm_map(train_corpus,removeWords,stopwords("english"))
train_corpus<-tm_map(train_corpus,removeWords,stopwordlist)
train_corpus<- tm_map(train_corpus,removePunctuation)
train_corpus<-tm_map(train_corpus,stripWhitespace)
if(boolstemm){
train_corpus<-tm_map(train_corpus,stemDocument,language = "english")
}
DTM <-DocumentTermMatrix(train_corpus,
control = list(tolower = T ,
removeNumbers =T ,
removePunctuation = T ,
stopwords = T
, stripWhitespace = T ,
dictionary = BagOfWord,
weighting = weightTfIdf))
print("DTM DONE !!")
print(DTM)
return(DTM)
}
binary.weight <-function(x)
{
x <- (x > 0)+0
}
exdata <- read.dir("/home/max/Desktop/ex/",".TXT")
exdata$content <- iconv(exdata$content,"WINDOWS-1252","UTF-8")
ex_corpus <- Corpus(VectorSource(as.matrix(exdata$content)))
setwd("/home/max/Desktop/Identification of the class of English text among several classes/analysis/scripts/")
ex_dtm <-preProcess_TFIDF(row.data = exdata$content ,stopword.dir = "test.txt",BagOfWord = BagOW , TRUE )
ex_dtm <-preProcess_TFIDF(row.data = exdata$content ,stopword.dir = "test.txt",BagOfWord = BagOW , TRUE )
dim(ex_dtm)
### test1 matrix form
ex_matrix <- as.matrix(ex_dtm)
ex_data_model <- as.data.frame(ex_matrix)
##test1_data_model
dim(ex_data_model)
ex_pred = predict(svm,newdata = ex_data_model )
library(caret)
library(doParallel)
cl <- makePSOCKcluster(5)
registerDoParallel(cl)
timr <- Sys.time()
svm <- train(y~.,data = trainData,method = 'svmLinear3')
## When you are done:
stopCluster(cl)
total_runtime <- difftime(Sys.time(), timr)
print(total_runtime)
ex_pred = predict(svm,newdata = ex_data_model )
ex_dtm <-preProcess_TFIDF(row.data = exdata$content ,stopword.dir = "test.txt",BagOfWord = BagOW , TRUE )
dim(ex_dtm)
### test1 matrix form
ex_matrix <- as.matrix(ex_dtm)
ex_data_model <- as.data.frame(ex_matrix)
##test1_data_model
dim(ex_data_model)
ex_pred = predict(svm,newdata = ex_data_model )
ex_data_model <- data.frame(x=ex_data_model)
ex_pred = predict(svm,newdata = ex_data_model )
for(i in 1:length(ex_corpus) ){
print( ex_corpus[[i]][["content"]] )
print(": ")
print( ex_pred[i])
}
